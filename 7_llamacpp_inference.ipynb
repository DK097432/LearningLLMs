{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:53:54.168589Z",
     "start_time": "2025-10-18T17:53:51.380255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/v1/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"SmolLM2-1.7B-Instruct-Q4_K_M\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain black holes in simple terms.\"}\n",
    "    ],\n",
    "    \"max_tokens\": 128,\n",
    "    \"temperature\": 0.7\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "print(\"\\n=== RAW RESPONSE ===\\n\")\n",
    "print(response.text)\n",
    "\n",
    "print(\"\\n=== Parsed Model Reply ===\\n\")\n",
    "\n",
    "try:\n",
    "    data = response.json()\n",
    "    print(data[\"choices\"][0][\"message\"][\"content\"])\n",
    "except Exception as e:\n",
    "    print(\"Failed to parse response:\", e)\n"
   ],
   "id": "28f4272a7295323c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAW RESPONSE ===\n",
      "\n",
      "{\"choices\":[{\"finish_reason\":\"length\",\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Black holes are regions in space where the gravitational pull is so strong that nothing, including light, can escape. They are formed when a massive star dies.\\n\\nImagine a star that is much bigger than our sun. At the end of its life, it explodes in a supernova, leaving behind a core. If this core is heavy enough, it will collapse into a black hole.\\n\\nThe black hole's gravity is so strong because it's pulling in everything around it. It's like a vacuum cleaner, sucking in everything it can get its tentacles around. This is why it's called a black hole - because it's invisible\"}}],\"created\":1760810034,\"model\":\"SmolLM2-1.7B-Instruct-Q4_K_M\",\"system_fingerprint\":\"b6792-81387858\",\"object\":\"chat.completion\",\"usage\":{\"completion_tokens\":128,\"prompt_tokens\":28,\"total_tokens\":156},\"id\":\"chatcmpl-BAWyrqTXJUN0nLrO74MR0DXuZlYyk3Tf\",\"timings\":{\"cache_n\":0,\"prompt_n\":28,\"prompt_ms\":157.526,\"prompt_per_token_ms\":5.625928571428572,\"prompt_per_second\":177.74843517895457,\"predicted_n\":128,\"predicted_ms\":673.318,\"predicted_per_token_ms\":5.260296875,\"predicted_per_second\":190.10333898692744}}\n",
      "\n",
      "=== Parsed Model Reply ===\n",
      "\n",
      "Black holes are regions in space where the gravitational pull is so strong that nothing, including light, can escape. They are formed when a massive star dies.\n",
      "\n",
      "Imagine a star that is much bigger than our sun. At the end of its life, it explodes in a supernova, leaving behind a core. If this core is heavy enough, it will collapse into a black hole.\n",
      "\n",
      "The black hole's gravity is so strong because it's pulling in everything around it. It's like a vacuum cleaner, sucking in everything it can get its tentacles around. This is why it's called a black hole - because it's invisible\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:58:36.017632Z",
     "start_time": "2025-10-18T17:58:14.238413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/v1/chat/completions\"\n",
    "\n",
    "def ask(prompt):\n",
    "    payload = {\n",
    "        \"model\": \"SmolLM2-1.7B-Instruct-Q4_K_M\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 2560,\n",
    "        \"temperature\": 0.7\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "print(\"=== SmolLM Chat ===\")\n",
    "while True:\n",
    "    prompt = input(\"\\nYou: \")\n",
    "    if prompt.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    reply = ask(prompt)\n",
    "    print(f\"AI: {reply}\")\n"
   ],
   "id": "fe80217b0dfa69e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SmolLM Chat ===\n",
      "AI: The Prophet Muhammad (c. 570 - 632 CE) is the founder of Islam and the last prophet of Allah in Islamic belief. He was born in Mecca, a city in present-day Saudi Arabia. His teachings form the basis of Islam, the world's second-largest religion.\n",
      "\n",
      "Prophet Muhammad was born into the Quraysh tribe, a prominent tribe in Mecca. His father, Abdullah, died before he was born, and his mother, Amina, passed away when he was six years old. Muhammad was then raised by his grandfather Abdul-Muttalib, who passed away when he was eight.\n",
      "\n",
      "At the age of 40, while meditating in a cave outside Mecca, Muhammad received his first revelation from God, which he later received over a period of two years. These revelations, known as the Quran, were later compiled into a book, and Muhammad became the last prophet of Allah.\n",
      "\n",
      "Prophet Muhammad preached peace and unity, calling people to monotheism and rejecting the idea of idols and polytheism. He also called upon people to treat each other with kindness and respect, and to fight against injustice.\n",
      "\n",
      "In 622 CE, Muhammad and his followers migrated from Mecca to the city of Medina, which is known as the \"Hegira\" or the Flight. This migration marked the beginning of the Islamic calendar. Over the next decade, Muhammad united the warring tribes of Arabia under one religion and one leader, establishing Islam as the dominant religion in the region.\n",
      "\n",
      "In 632 CE, when Muhammad was 63, he passed away at Medina. His teachings were compiled into the Quran and his followers continued to spread his message throughout the Middle East, North Africa, and parts of Europe.\n",
      "\n",
      "Prophet Muhammad's legacy is revered across the Islamic world, and he is considered to be the perfect example of a Muslim. Muslims revere him as the last of the prophets and the founder of their faith. They believe his teachings will continue to guide them in their lives as they strive to live in accordance with the principles of Islam.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 20\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=== SmolLM Chat ===\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 20\u001B[0m     prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43mYou: \u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m prompt\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquit\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m     22\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\ipykernel\\kernelbase.py:1275\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[1;34m(self, prompt)\u001B[0m\n\u001B[0;32m   1273\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1274\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[1;32m-> 1275\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1276\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1277\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1278\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1279\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1280\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\ipykernel\\kernelbase.py:1320\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[1;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[0;32m   1317\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[0;32m   1318\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[0;32m   1319\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1320\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1321\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1322\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T17:57:53.009023Z",
     "start_time": "2025-10-18T17:57:49.655130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"not-needed\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"SmolLM2-1.7B-Instruct-Q4_K_M\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Summarize black holes.\"}\n",
    "    ],\n",
    "    max_tokens=128,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ],
   "id": "3816ef1d2c126c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black holes are extremely dense regions in space where gravity is so strong that nothing, not even light, can escape once it falls within a certain boundary called the event horizon. They are formed when a massive star collapses under its own gravity. Once a black hole is formed, its gravity pulls in nearby matter and energy, causing the black hole to grow larger. Black holes are invisible because they don't emit any light, but they can be detected through their effects on the surrounding environment, such as the movement of stars and gas.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:02:08.407740Z",
     "start_time": "2025-10-18T18:02:05.486781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize client pointing to llama.cpp server\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8080/v1\",  # URL to the llama.cpp server\n",
    "    token=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
    ")\n",
    "\n",
    "# Text generation\n",
    "response = client.text_generation(\n",
    "    \"Tell me a story\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    ")\n",
    "print(response.generated_text)\n",
    "\n",
    "# For chat format\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "7627b6a53034a973",
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "404 Client Error: Not Found for url: http://localhost:8080/v1\n\n{'code': 404, 'message': 'File Not Found', 'type': 'not_found_error'}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:407\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 407\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\requests\\models.py:1026\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1025\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[1;32m-> 1026\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPError\u001B[0m: 404 Client Error: Not Found for url: http://localhost:8080/v1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 10\u001B[0m\n\u001B[0;32m      4\u001B[0m client \u001B[38;5;241m=\u001B[39m InferenceClient(\n\u001B[0;32m      5\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp://localhost:8080/v1\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \u001B[38;5;66;03m# URL to the llama.cpp server\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     token\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msk-no-key-required\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \u001B[38;5;66;03m# llama.cpp server requires this placeholder\u001B[39;00m\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Text generation\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtext_generation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTell me a story\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.7\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.95\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdetails\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     16\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mgenerated_text)\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# For chat format\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2413\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[0;32m   2388\u001B[0m         _set_unsupported_text_generation_kwargs(model, unused_params)\n\u001B[0;32m   2389\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtext_generation(  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m   2390\u001B[0m             prompt\u001B[38;5;241m=\u001B[39mprompt,\n\u001B[0;32m   2391\u001B[0m             details\u001B[38;5;241m=\u001B[39mdetails,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2411\u001B[0m             watermark\u001B[38;5;241m=\u001B[39mwatermark,\n\u001B[0;32m   2412\u001B[0m         )\n\u001B[1;32m-> 2413\u001B[0m     \u001B[43mraise_text_generation_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2415\u001B[0m \u001B[38;5;66;03m# Parse output\u001B[39;00m\n\u001B[0;32m   2416\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\inference\\_common.py:447\u001B[0m, in \u001B[0;36mraise_text_generation_error\u001B[1;34m(http_error)\u001B[0m\n\u001B[0;32m    444\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exception \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mhttp_error\u001B[39;00m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;66;03m# Otherwise, fallback to default error\u001B[39;00m\n\u001B[1;32m--> 447\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m http_error\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:2383\u001B[0m, in \u001B[0;36mInferenceClient.text_generation\u001B[1;34m(self, prompt, details, stream, model, adapter_id, best_of, decoder_input_details, do_sample, frequency_penalty, grammar, max_new_tokens, repetition_penalty, return_full_text, seed, stop, stop_sequences, temperature, top_k, top_n_tokens, top_p, truncate, typical_p, watermark)\u001B[0m\n\u001B[0;32m   2381\u001B[0m \u001B[38;5;66;03m# Handle errors separately for more precise error messages\u001B[39;00m\n\u001B[0;32m   2382\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 2383\u001B[0m     bytes_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inner_post\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest_parameters\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2384\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   2385\u001B[0m     match \u001B[38;5;241m=\u001B[39m MODEL_KWARGS_NOT_USED_REGEX\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:275\u001B[0m, in \u001B[0;36mInferenceClient._inner_post\u001B[1;34m(self, request_parameters, stream)\u001B[0m\n\u001B[0;32m    272\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrequest_parameters\u001B[38;5;241m.\u001B[39murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 275\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[0;32m    277\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:480\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[1;34m(response, endpoint_name)\u001B[0m\n\u001B[0;32m    476\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, message, response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[1;32m--> 480\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m _format(HfHubHTTPError, \u001B[38;5;28mstr\u001B[39m(e), response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[1;31mHfHubHTTPError\u001B[0m: 404 Client Error: Not Found for url: http://localhost:8080/v1\n\n{'code': 404, 'message': 'File Not Found', 'type': 'not_found_error'}"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:03:44.634931Z",
     "start_time": "2025-10-18T18:03:41.743625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"not-needed\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"SmolLM2-1.7B-Instruct-Q4_K_M\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ],
   "id": "baa4248408314eaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a young girl named Lily who lived in a small village at the edge of a vast, enchanted forest. The villagers often whispered about the forest, claiming it was home to magical creatures, and that its ancient trees held the secrets of the past.\n",
      "\n",
      "One day, Lily decided to explore the forest, feeling an inexplicable pull towards it. She ventured deeper, her heart pounding with excitement, and soon found herself at the foot of a towering tree.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:04:03.244661Z",
     "start_time": "2025-10-18T18:04:00.494524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8080/v1/chat/completions\"\n",
    "data = {\n",
    "    \"model\": \"SmolLM2-1.7B-Instruct-Q4_K_M\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 100\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n"
   ],
   "id": "4e0eb543cdfece38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of the mystical forest, there lived a young girl named Luna. She was a curious and adventurous soul, always seeking new tales and hidden wonders. Luna lived with her wise grandmother, who was known throughout the forest for her extraordinary tales and knowledge of the ancient magic that dwelled within it.\n",
      "\n",
      "One day, while Luna was exploring the forest, she stumbled upon a hidden cave. She cautiously entered the cave and found an ancient scroll on a stone pedestal. The scroll was old\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:53:19.416058Z",
     "start_time": "2025-10-18T18:53:19.412058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "url= 'http://localhost:8080/v1/chat/completions'"
   ],
   "id": "3dcba07d93635d45",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:53:21.158638Z",
     "start_time": "2025-10-18T18:53:21.154639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "payload = {\n",
    "    'model': 'SmolLM2-1.7B-Instruct-Q4_K_M',\n",
    "    'messages':[\n",
    "        {'role':'system','content':'You are a great historian'},\n",
    "        {'role':'user','content':'Tell me about the greatest roman emperors'}\n",
    "    ],\n",
    "    'max_tokens':1000,\n",
    "    'temperature':0.8\n",
    "}"
   ],
   "id": "245be143da85650b",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:53:22.376839Z",
     "start_time": "2025-10-18T18:53:22.363324Z"
    }
   },
   "cell_type": "code",
   "source": "response = requests.post(url,json=payload)",
   "id": "15c117b5801f280",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:53:44.690078Z",
     "start_time": "2025-10-18T18:53:44.665055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Here is your Answer: \\n')\n",
    "data = response.json()\n",
    "print(data['choices'][0]['message']['content'])"
   ],
   "id": "472179ee4ddb2d98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is your Answer: \n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHere is your Answer: \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mchoices\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtext)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'choices'"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-18T18:54:16.230480Z",
     "start_time": "2025-10-18T18:54:15.776420Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d30a55f520d99e1b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Quantum computing makes use of quantum mechanics principles to solve complex problems in a significantly faster manner compared to traditional computers. It utilizes qubits as it does not require the classical bits to be fixed, allowing for more flexibility in terms of information manipulation.\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "66d8978d4e59c9e0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
