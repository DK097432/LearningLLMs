{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T15:40:05.686979Z",
     "start_time": "2025-10-29T15:40:05.682982Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:18:30.217545Z",
     "start_time": "2025-10-29T15:18:24.049143Z"
    }
   },
   "cell_type": "code",
   "source": "question_answerer = pipeline('question-answering')",
   "id": "34d41c83b3d3ba2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:19:51.093501Z",
     "start_time": "2025-10-29T15:19:51.089990Z"
    }
   },
   "cell_type": "code",
   "source": "question = \"Which deep learning libraries back  Transformers?\"",
   "id": "a5e10f4808d5473c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:19:51.360905Z",
     "start_time": "2025-10-29T15:19:51.354908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "long_context = \"\"\"\n",
    "Transformers: State of the Art NLP\n",
    "\n",
    " Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    " Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    " Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\""
   ],
   "id": "4530baee5fe6a5c6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:20:09.405142Z",
     "start_time": "2025-10-29T15:20:09.025643Z"
    }
   },
   "cell_type": "code",
   "source": "question_answerer(question = question, context = long_context)",
   "id": "c8de448c996cccdc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9700287699743058,\n",
       " 'start': 1887,\n",
       " 'end': 1914,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:21:48.786690Z",
     "start_time": "2025-10-29T15:21:46.807718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "checkpoint = 'distilbert-base-cased-distilled-squad'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)"
   ],
   "id": "dd7ad8e6e514b74a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:22:16.824626Z",
     "start_time": "2025-10-29T15:22:16.819110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(question, long_context)\n",
    "print(len(inputs['input_ids'])) # The length is 456 which is longer than the context length of model (384)"
   ],
   "id": "461d97e31ebb2684",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:23:35.935027Z",
     "start_time": "2025-10-29T15:23:35.928513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(question, long_context, max_length = 384, truncation = 'only_second')# Truncation is 'only_second' because the context is from second line\n",
    "print(tokenizer.decode(inputs['input_ids']))"
   ],
   "id": "e05b4aba0658c4f3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Which deep learning libraries back Transformers? [SEP] Transformers : State of the Art NLP Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages. Its aim is to make cutting - edge NLP easier to use for everyone. Transformers provides APIs to quickly download and use those pretrained models on a given text, fine - tune them on your own datasets and then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments. Why should I use transformers? 1. Easy - to - use state - of - the - art models : - High performance on NLU and NLG tasks. - Low barrier to entry for educators and practitioners. - Few user - facing abstractions with just three classes to learn. - A unified API for using all our pretrained models. - Lower compute costs, smaller carbon footprint : 2. Researchers can share trained models instead of always retraining. - Practitioners can reduce compute time and production costs. - Dozens of architectures with over 10, 000 pretrained models, some in more than 100 languages. 3. Choose the right framework for every part of a model ' s lifetime : - Train state - of - the - art models in 3 lines of code. - Move a single model between TF2. 0 / PyTorch frameworks at will. - Seamlessly pick the right framework for training, evaluation and production. 4. Easily customize a model or an example to your needs : - We provide examples for each architecture to reproduce the results published by its original authors. - Model internals are exposed as [SEP]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:27:31.415678Z",
     "start_time": "2025-10-29T15:27:31.409676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To make sure that no important part of the context is left out we will make chunks of the long context and for that we will return overflowing tokens to True\n",
    "sentence = 'We will be splitting this sentence as an example although it is not very long'\n",
    "inputs = tokenizer(\n",
    "    sentence, truncation=True, return_overflowing_tokens=True, max_length=5,stride = 2# stride defines the overlap between the splitted new sentence\n",
    ")\n",
    "for ids in inputs['input_ids']:\n",
    "    print(tokenizer.decode(ids))"
   ],
   "id": "82cc6f4480b25d95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] We will be [SEP]\n",
      "[CLS] will be splitting [SEP]\n",
      "[CLS] be splitting this [SEP]\n",
      "[CLS] splitting this sentence [SEP]\n",
      "[CLS] this sentence as [SEP]\n",
      "[CLS] sentence as an [SEP]\n",
      "[CLS] as an example [SEP]\n",
      "[CLS] an example although [SEP]\n",
      "[CLS] example although it [SEP]\n",
      "[CLS] although it is [SEP]\n",
      "[CLS] it is not [SEP]\n",
      "[CLS] is not very [SEP]\n",
      "[CLS] not very long [SEP]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:28:47.425983Z",
     "start_time": "2025-10-29T15:28:47.420928Z"
    }
   },
   "cell_type": "code",
   "source": "print(inputs.keys())",
   "id": "af5e61dc1878b1ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': [[101, 1284, 1209, 1129, 102], [101, 1209, 1129, 15601, 102], [101, 1129, 15601, 1142, 102], [101, 15601, 1142, 5650, 102], [101, 1142, 5650, 1112, 102], [101, 5650, 1112, 1126, 102], [101, 1112, 1126, 1859, 102], [101, 1126, 1859, 1780, 102], [101, 1859, 1780, 1122, 102], [101, 1780, 1122, 1110, 102], [101, 1122, 1110, 1136, 102], [101, 1110, 1136, 1304, 102], [101, 1136, 1304, 1263, 102]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]})\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:29:35.305995Z",
     "start_time": "2025-10-29T15:29:35.301990Z"
    }
   },
   "cell_type": "code",
   "source": "print(inputs['overflow_to_sample_mapping']) # here we see 13 zeros which are due to 13 sentences after the split of original sentence",
   "id": "8e4a292526fd3bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:32:31.555063Z",
     "start_time": "2025-10-29T15:32:31.550548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# another example where we split multiple sentences\n",
    "sentences = ['Hey there! how are you?',\n",
    "             'Large Language Models are everywhere',\n",
    "             'Technology is growing lightning fast']\n",
    "\n",
    "inputs = tokenizer(sentences, truncation=True, return_overflowing_tokens=True, max_length=6, stride = 2)\n",
    "\n",
    "print(inputs['overflow_to_sample_mapping'])"
   ],
   "id": "9d1708148a5452db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 2, 2]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Back to Long Context",
   "id": "5ff111032f5470db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:34:10.189820Z",
     "start_time": "2025-10-29T15:34:10.183299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride = 128,\n",
    "    max_length = 384,\n",
    "    padding= 'longest',\n",
    "    truncation = 'only_second',\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    ")"
   ],
   "id": "4a8744db693b22b9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:35:05.230935Z",
     "start_time": "2025-10-29T15:35:05.224899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_ = inputs.pop('overflow_to_sample_mapping')\n",
    "offsets = inputs.pop('offset_mapping')\n",
    "\n",
    "inputs = inputs.convert_to_tensors('pt')\n",
    "print(inputs['input_ids'].shape)"
   ],
   "id": "32a9f68d27b4a92c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:36:22.499730Z",
     "start_time": "2025-10-29T15:36:21.803614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we will have 2 sets of start and end logits\n",
    "outputs = model(**inputs)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ],
   "id": "92d879a9d48e86dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 384]) torch.Size([2, 384])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:40:14.451972Z",
     "start_time": "2025-10-29T15:40:14.444819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We mask the tokens that are not part of the Context and we also mask the padding tokens\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# mask everything apart from the tokens of the context\n",
    "mask = [i !=1 for i in sequence_ids]\n",
    "# Unmask the CLS token\n",
    "mask[0] = False\n",
    "# mask all the [PAD] tokens\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs['attention_mask']== 0))\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ],
   "id": "1c9cba495fa8c4a1",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:41:06.649260Z",
     "start_time": "2025-10-29T15:41:06.640748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we apply softmax to convert logits to probabilities\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)"
   ],
   "id": "d9dbc5d21e93099",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:44:31.846551Z",
     "start_time": "2025-10-29T15:44:31.833524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We'll attribute a score to all possible spans of answer and then take the span with best score\n",
    "candidate_answers = []\n",
    "for start_probs, end_probs in zip(start_probabilities, end_probabilities):\n",
    "    scores = start_probs[:,None]*end_probs[None,:]\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "\n",
    "    start_idx = idx//scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx,end_idx].item()\n",
    "\n",
    "    candidate_answers.append((start_idx, end_idx, score))"
   ],
   "id": "5d5fa094c9a87c6f",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:44:37.163330Z",
     "start_time": "2025-10-29T15:44:37.159818Z"
    }
   },
   "cell_type": "code",
   "source": "print(candidate_answers)",
   "id": "1afbdf2a093883ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0.6493729948997498), (167, 178, 0.9697459936141968)]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:46:51.654026Z",
     "start_time": "2025-10-29T15:46:51.647514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for candidate, offset in zip(candidate_answers, offsets):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char,_ = offset[start_token]\n",
    "    _, end_char= offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\n",
    "        'answer': answer,\n",
    "        'start': start_char,\n",
    "        'end': end_char,\n",
    "        'score': score,\n",
    "    }\n",
    "    print(result)"
   ],
   "id": "80f3e7a32a417d91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '', 'start': 0, 'end': 0, 'score': 0.6493729948997498}\n",
      "{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1887, 'end': 1914, 'score': 0.9697459936141968}\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "35ad901938eae80d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
