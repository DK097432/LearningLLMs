{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T19:23:18.536965Z",
     "start_time": "2025-10-15T19:23:12.063803Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:24:03.940143Z",
     "start_time": "2025-10-15T19:24:03.936130Z"
    }
   },
   "cell_type": "code",
   "source": "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'",
   "id": "404ca12bf3255e7f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:24:35.254126Z",
     "start_time": "2025-10-15T19:24:31.212097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ],
   "id": "7f147e24630e399f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:25:50.989183Z",
     "start_time": "2025-10-15T19:25:50.986185Z"
    }
   },
   "cell_type": "code",
   "source": "sentence = 'I am going to become one of the greatest men ever lived on planet Earth'",
   "id": "745370ea8c5f7297",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:26:06.329109Z",
     "start_time": "2025-10-15T19:26:06.325096Z"
    }
   },
   "cell_type": "code",
   "source": "tokens = tokenizer.tokenize(sentence)",
   "id": "c7c2822705234f56",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:26:29.092544Z",
     "start_time": "2025-10-15T19:26:29.088028Z"
    }
   },
   "cell_type": "code",
   "source": "token_ids = tokenizer.convert_tokens_to_ids(tokens)",
   "id": "2a16c2e86a0d3b62",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:26:42.930916Z",
     "start_time": "2025-10-15T19:26:42.926912Z"
    }
   },
   "cell_type": "code",
   "source": "input_ids = torch.tensor(token_ids)",
   "id": "315f506d83fff3d9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:27:00.502014Z",
     "start_time": "2025-10-15T19:26:59.290602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This Line will fail\n",
    "model(input_ids)# It is because transformer models expects multiple sequences by default but we sent only one"
   ],
   "id": "2dbad1dc7811cf6e",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# This Line will fail\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:905\u001B[0m, in \u001B[0;36mDistilBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    898\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m    899\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m    902\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    903\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m--> 905\u001B[0m distilbert_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistilbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    907\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    908\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    909\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    910\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    911\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    912\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    913\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    914\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m distilbert_output[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[0;32m    915\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m hidden_state[:, \u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:698\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    696\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    697\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 698\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarn_if_padding_and_no_attention_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    699\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m    700\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\modeling_utils.py:5705\u001B[0m, in \u001B[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m   5702\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   5704\u001B[0m \u001B[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001B[39;00m\n\u001B[1;32m-> 5705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;129;01min\u001B[39;00m \u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m:\n\u001B[0;32m   5706\u001B[0m     warn_string \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   5707\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5708\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5709\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5710\u001B[0m     )\n\u001B[0;32m   5712\u001B[0m     \u001B[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001B[39;00m\n\u001B[0;32m   5713\u001B[0m     \u001B[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:29:31.974747Z",
     "start_time": "2025-10-15T19:29:31.967491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_inputs = tokenizer(sentence, return_tensors='pt')\n",
    "print(tokenized_inputs['input_ids'])# Now this did not give error its is because it added a dimension on top of the converted list"
   ],
   "id": "ef7ade969d504d23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1045, 2572, 2183, 2000, 2468, 2028, 1997, 1996, 4602, 2273, 2412,\n",
      "         2973, 2006, 4774, 3011,  102]])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:31:04.602120Z",
     "start_time": "2025-10-15T19:31:04.596124Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_inputs['input_ids'].shape",
   "id": "f0d9cbfae7efc0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:34:04.744074Z",
     "start_time": "2025-10-15T19:34:02.207559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ],
   "id": "615057d6be12b367",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:35:27.929212Z",
     "start_time": "2025-10-15T19:35:27.924707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = 'I will become one of the greatest men ever lived on planet Earth'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)"
   ],
   "id": "85ba8b7f04671b9a",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:35:56.319154Z",
     "start_time": "2025-10-15T19:35:56.315151Z"
    }
   },
   "cell_type": "code",
   "source": "input_ids = torch.tensor([ids])# now it did not give error because we added one dimension to the list(ids) which is converted to tensor '[]'",
   "id": "59b192e62ae562c8",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:37:07.910420Z",
     "start_time": "2025-10-15T19:37:07.905906Z"
    }
   },
   "cell_type": "code",
   "source": "print('Input_IDs', input_ids)",
   "id": "daa170fab3cb223f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_IDs tensor([[1045, 2097, 2468, 2028, 1997, 1996, 4602, 2273, 2412, 2973, 2006, 4774,\n",
      "         3011]])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:37:35.555342Z",
     "start_time": "2025-10-15T19:37:35.308977Z"
    }
   },
   "cell_type": "code",
   "source": "output = model(input_ids)",
   "id": "b9771673e0e273d7",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:37:54.255899Z",
     "start_time": "2025-10-15T19:37:54.250895Z"
    }
   },
   "cell_type": "code",
   "source": "print('logits: ', output.logits)",
   "id": "d33ebf41bd4e5f82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  tensor([[-4.0116,  4.3434]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:44:43.854424Z",
     "start_time": "2025-10-15T19:44:43.850363Z"
    }
   },
   "cell_type": "code",
   "source": "batched_ids= [ids,ids]",
   "id": "8cc1bcb286620d19",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:45:12.366486Z",
     "start_time": "2025-10-15T19:45:12.361490Z"
    }
   },
   "cell_type": "code",
   "source": "batched_ids_tensor = torch.tensor(batched_ids)",
   "id": "c01eaf4d151c28e3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:45:31.582264Z",
     "start_time": "2025-10-15T19:45:31.576252Z"
    }
   },
   "cell_type": "code",
   "source": "batched_ids_tensor.shape",
   "id": "aee1e88857f0d730",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:46:20.594721Z",
     "start_time": "2025-10-15T19:46:20.554106Z"
    }
   },
   "cell_type": "code",
   "source": "ouput_batched_ids = model(batched_ids_tensor)",
   "id": "3f2c28e495b6e803",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:46:57.213366Z",
     "start_time": "2025-10-15T19:46:57.208843Z"
    }
   },
   "cell_type": "code",
   "source": "print('Batched_ids_logit: ', ouput_batched_ids.logits) # It gives the same logits as above but doubled hence proved batching does not change result",
   "id": "7a87458404484d99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched_ids_logit:  tensor([[-4.0116,  4.3434],\n",
      "        [-4.0116,  4.3434]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Padding and Markdown",
   "id": "6e2176e2ac01caae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:49:44.256018Z",
     "start_time": "2025-10-15T19:49:44.251851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batched_ids = [\n",
    "    [200,200,200],\n",
    "    [200,200]\n",
    "]"
   ],
   "id": "ca46d42eb8b39cc2",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:50:21.852808Z",
     "start_time": "2025-10-15T19:50:21.848832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we will have to use padding to make the shape rectangular\n",
    "padding_id = 100\n",
    "batched_ids = [\n",
    "    [200,200,200],\n",
    "    [200,200,padding_id]\n",
    "]"
   ],
   "id": "28bfbe5e20972990",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:51:44.376996Z",
     "start_time": "2025-10-15T19:51:43.807684Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)",
   "id": "43a02d1ecc0ea3f2",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:56:27.736293Z",
     "start_time": "2025-10-15T19:56:27.732293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sequence_ids1 = [[200,200,200]]\n",
    "sequence_ids2 = [[200,200]]\n",
    "batched_ids = [\n",
    "    [200,200,200],\n",
    "    [200,200,tokenizer.pad_token_id]\n",
    "]"
   ],
   "id": "16dd51d68b9677cc",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:56:30.728338Z",
     "start_time": "2025-10-15T19:56:30.651400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(model(torch.tensor(sequence_ids1)).logits)\n",
    "print(model(torch.tensor(sequence_ids2)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits) # The logits of the second row will be different because we donot have atention layers here like in transformers which would have ignored the padding"
   ],
   "id": "fe6d1ebdfe6b7090",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T19:59:50.843330Z",
     "start_time": "2025-10-15T19:59:50.838329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Attention Masks\n",
    "batched_ids = [\n",
    "    [200,200,200],\n",
    "    [200,200,tokenizer.pad_token_id]\n",
    "]"
   ],
   "id": "2d0224e07645d6a4",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:00:15.659345Z",
     "start_time": "2025-10-15T20:00:15.655337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "attention_mask = [\n",
    "    [1,1,1],\n",
    "    [1,1,0]\n",
    "]"
   ],
   "id": "b1e4ee9a9349d692",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:01:25.232732Z",
     "start_time": "2025-10-15T20:01:25.200617Z"
    }
   },
   "cell_type": "code",
   "source": "outputs = model(torch.tensor(batched_ids), attention_mask = torch.tensor(attention_mask))",
   "id": "3929f95ba4bc5036",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:01:50.126323Z",
     "start_time": "2025-10-15T20:01:50.120324Z"
    }
   },
   "cell_type": "code",
   "source": "print('Output lohits: ', outputs.logits)# now we would have correcrt logits after applying the attetion mask",
   "id": "fc95bba2ae6e5abd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output lohits:  tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:03:41.074010Z",
     "start_time": "2025-10-15T20:03:41.069505Z"
    }
   },
   "cell_type": "code",
   "source": "sentence",
   "id": "f59b6091335b7a4f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will become one of the greatest men ever lived on planet Earth'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:03:52.499457Z",
     "start_time": "2025-10-15T20:03:52.496156Z"
    }
   },
   "cell_type": "code",
   "source": "tokens = tokenizer.tokenize(sentence)",
   "id": "df1c8c9d1bced9fd",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:04:07.369049Z",
     "start_time": "2025-10-15T20:04:07.363011Z"
    }
   },
   "cell_type": "code",
   "source": "tokens",
   "id": "a2d11d6849770719",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'will',\n",
       " 'become',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'men',\n",
       " 'ever',\n",
       " 'lived',\n",
       " 'on',\n",
       " 'planet',\n",
       " 'earth']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:04:24.716301Z",
     "start_time": "2025-10-15T20:04:24.710797Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokens)",
   "id": "7ed15e4103203b21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:04:44.732716Z",
     "start_time": "2025-10-15T20:04:44.729201Z"
    }
   },
   "cell_type": "code",
   "source": "sentence2 = 'I am learning LLMs'",
   "id": "b861a6d216a8b42e",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:05:05.947353Z",
     "start_time": "2025-10-15T20:05:05.943840Z"
    }
   },
   "cell_type": "code",
   "source": "tokens2 = tokenizer.tokenize(sentence2)",
   "id": "7a309a6e39cf7004",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:05:13.901904Z",
     "start_time": "2025-10-15T20:05:13.896386Z"
    }
   },
   "cell_type": "code",
   "source": "len(tokens2)",
   "id": "4e50906facde44d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:09:11.128257Z",
     "start_time": "2025-10-15T20:09:11.124256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ids1 = tokenizer.convert_tokens_to_ids(tokens)\n",
    "ids2 = tokenizer.convert_tokens_to_ids(tokens2)"
   ],
   "id": "b57f966bcbf2d538",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:09:55.961877Z",
     "start_time": "2025-10-15T20:09:55.892247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('sentence1 logits: ', model(torch.tensor([ids1])).logits)\n",
    "print('sentence2 logits: ', model(torch.tensor([ids2])).logits)"
   ],
   "id": "1233fb88263f981a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 logits:  tensor([[-4.0116,  4.3434]], grad_fn=<AddmmBackward0>)\n",
      "sentence2 logits:  tensor([[ 1.0627, -0.9045]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:11:52.115582Z",
     "start_time": "2025-10-15T20:11:52.110582Z"
    }
   },
   "cell_type": "code",
   "source": "print(ids[0])",
   "id": "57882042408ac837",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:14:24.353457Z",
     "start_time": "2025-10-15T20:14:24.349259Z"
    }
   },
   "cell_type": "code",
   "source": "batched_ids = [ids1[0],ids2[0]]\n",
   "id": "91a3d1e0fcbd03ba",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:14:33.473222Z",
     "start_time": "2025-10-15T20:14:33.468707Z"
    }
   },
   "cell_type": "code",
   "source": "batched_ids_tensor = torch.tensor(batched_ids)",
   "id": "6a7680df42fdc41f",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:14:34.354631Z",
     "start_time": "2025-10-15T20:14:34.350037Z"
    }
   },
   "cell_type": "code",
   "source": "batched_ids_tensor.shape",
   "id": "91cb04aa5d5de7ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-15T20:14:35.117622Z",
     "start_time": "2025-10-15T20:14:35.007720Z"
    }
   },
   "cell_type": "code",
   "source": "print('Batched Ids Logits: ', model(batched_ids_tensor).logits)",
   "id": "f37b2fd5badf4301",
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[76], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBatched Ids Logits: \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatched_ids_tensor\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlogits)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:905\u001B[0m, in \u001B[0;36mDistilBertForSequenceClassification.forward\u001B[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    897\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    898\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m    899\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m    902\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    903\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m--> 905\u001B[0m distilbert_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistilbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    906\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    907\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    908\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    909\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    910\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    911\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    912\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    913\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    914\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m distilbert_output[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[0;32m    915\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m hidden_state[:, \u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:698\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    696\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    697\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m input_ids \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 698\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwarn_if_padding_and_no_attention_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    699\u001B[0m     input_shape \u001B[38;5;241m=\u001B[39m input_ids\u001B[38;5;241m.\u001B[39msize()\n\u001B[0;32m    700\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mc:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\transformers\\modeling_utils.py:5705\u001B[0m, in \u001B[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001B[1;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[0;32m   5702\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   5704\u001B[0m \u001B[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001B[39;00m\n\u001B[1;32m-> 5705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpad_token_id \u001B[38;5;129;01min\u001B[39;00m \u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m:\n\u001B[0;32m   5706\u001B[0m     warn_string \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   5707\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5708\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5709\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   5710\u001B[0m     )\n\u001B[0;32m   5712\u001B[0m     \u001B[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001B[39;00m\n\u001B[0;32m   5713\u001B[0m     \u001B[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1fc193aa2949745c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
