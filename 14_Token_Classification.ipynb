{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T17:26:36.887626Z",
     "start_time": "2025-10-28T17:26:26.858577Z"
    }
   },
   "source": "from transformers import pipeline",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:26:46.471661Z",
     "start_time": "2025-10-28T17:26:41.735325Z"
    }
   },
   "cell_type": "code",
   "source": "token_classifier = pipeline('token-classification')",
   "id": "821b56330a550aa9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:28:03.203089Z",
     "start_time": "2025-10-28T17:28:03.075142Z"
    }
   },
   "cell_type": "code",
   "source": "token_classifier('I am Dawood Khan and I live in Munich')",
   "id": "210491e27f03cb84",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': np.float32(0.999718),\n",
       "  'index': 3,\n",
       "  'word': 'Da',\n",
       "  'start': 5,\n",
       "  'end': 7},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99953854),\n",
       "  'index': 4,\n",
       "  'word': '##wood',\n",
       "  'start': 7,\n",
       "  'end': 11},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99973756),\n",
       "  'index': 5,\n",
       "  'word': 'Khan',\n",
       "  'start': 12,\n",
       "  'end': 16},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': np.float32(0.998351),\n",
       "  'index': 10,\n",
       "  'word': 'Munich',\n",
       "  'start': 31,\n",
       "  'end': 37}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:37:07.102063Z",
     "start_time": "2025-10-28T17:37:04.498464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_classifier = pipeline('token-classification', aggregation_strategy='average')\n",
    "token_classifier('I am Dawood and I work in Munich')"
   ],
   "id": "6b669dead8874ef1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9993456),\n",
       "  'word': 'Dawood',\n",
       "  'start': 5,\n",
       "  'end': 11},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9986532),\n",
       "  'word': 'Munich',\n",
       "  'start': 26,\n",
       "  'end': 32}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Now we'll do all the work manually",
   "id": "e385efe411d79997"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### From Inputs to Predictions",
   "id": "b821c431b205606b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:43:12.580700Z",
     "start_time": "2025-10-28T17:43:11.685002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First tokenize input and pass it through model\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "checkpoint = 'dbmdz/bert-large-cased-finetuned-conll03-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')"
   ],
   "id": "fd75ec660a6f1a9c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:43:25.729434Z",
     "start_time": "2025-10-28T17:43:24.608716Z"
    }
   },
   "cell_type": "code",
   "source": "model = AutoModelForTokenClassification.from_pretrained(checkpoint)",
   "id": "a7b99d7a4f03f3c8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:44:25.649481Z",
     "start_time": "2025-10-28T17:44:25.091968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentence = 'Dawood was working for UN in Pakistan'\n",
    "inputs = tokenizer(sentence, return_tensors='pt')\n",
    "outputs = model(**inputs)"
   ],
   "id": "b0d2cc332f2dd44b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:44:56.278091Z",
     "start_time": "2025-10-28T17:44:56.274087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(inputs['input_ids'].shape)\n",
    "print(outputs.logits.shape# Batch of 1 sequence, 10 tokens and 9 different labels ([1,10,9])"
   ],
   "id": "65006a2e35f0f859",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([1, 10, 9])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:48:19.028452Z",
     "start_time": "2025-10-28T17:48:19.019451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We use softmax to convert logits into probabilities\n",
    "import torch\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0].tolist()\n",
    "predictions = outputs.logits.argmax(dim=-1)[0].tolist()"
   ],
   "id": "5c386400fd2d2b41",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:48:28.819411Z",
     "start_time": "2025-10-28T17:48:28.815410Z"
    }
   },
   "cell_type": "code",
   "source": "print(predictions)",
   "id": "5b7a3dbc819fce39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 4, 0, 0, 0, 6, 0, 8, 0]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:48:46.001703Z",
     "start_time": "2025-10-28T17:48:45.995703Z"
    }
   },
   "cell_type": "code",
   "source": "model.config.id2label",
   "id": "26f158b634021ff5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-MISC',\n",
       " 2: 'I-MISC',\n",
       " 3: 'B-PER',\n",
       " 4: 'I-PER',\n",
       " 5: 'B-ORG',\n",
       " 6: 'I-ORG',\n",
       " 7: 'B-LOC',\n",
       " 8: 'I-LOC'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:49:28.897452Z",
     "start_time": "2025-10-28T17:49:28.893449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = []\n",
    "tokens = inputs.tokens()"
   ],
   "id": "83fdf4a82fcdf8ad",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:51:23.433643Z",
     "start_time": "2025-10-28T17:51:23.427644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for idx ,pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label!=0:\n",
    "        results.append(\n",
    "            {'entity':label, 'score':probabilities[idx][pred],'word':tokens[idx]}\n",
    "        )\n",
    "\n",
    "print(results)"
   ],
   "id": "27ebb82cf2f7c2b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'O', 'score': 0.9996232986450195, 'word': '[CLS]'}, {'entity': 'I-PER', 'score': 0.999491810798645, 'word': 'Da'}, {'entity': 'I-PER', 'score': 0.999091625213623, 'word': '##wood'}, {'entity': 'O', 'score': 0.999958872795105, 'word': 'was'}, {'entity': 'O', 'score': 0.9999486207962036, 'word': 'working'}, {'entity': 'O', 'score': 0.9998605251312256, 'word': 'for'}, {'entity': 'I-ORG', 'score': 0.9988266825675964, 'word': 'UN'}, {'entity': 'O', 'score': 0.9997767806053162, 'word': 'in'}, {'entity': 'I-LOC', 'score': 0.9998264908790588, 'word': 'Pakistan'}, {'entity': 'O', 'score': 0.9996232986450195, 'word': '[SEP]'}]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:53:06.773702Z",
     "start_time": "2025-10-28T17:53:06.766702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now we do offsets mapping to get the info about start and end of each entity\n",
    "inputs_offsets = tokenizer(sentence,return_offsets_mapping = True)\n",
    "inputs_offsets['offset_mapping']"
   ],
   "id": "84d97963c6701a06",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (2, 6),\n",
       " (7, 10),\n",
       " (11, 18),\n",
       " (19, 22),\n",
       " (23, 25),\n",
       " (26, 28),\n",
       " (29, 37),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:53:46.226622Z",
     "start_time": "2025-10-28T17:53:46.220623Z"
    }
   },
   "cell_type": "code",
   "source": "sentence[7:10]",
   "id": "105db8fff0e974df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'was'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:57:30.168006Z",
     "start_time": "2025-10-28T17:57:30.162007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# using this we can complete our results just like 'pipeline' function\n",
    "results = []\n",
    "inputs_offsets = tokenizer(sentence,return_offsets_mapping = True)\n",
    "tokens = inputs_offsets.tokens()\n",
    "offsets = inputs_offsets['offset_mapping']\n",
    "for idx ,pred in enumerate(predictions):\n",
    "    label = model.config.id2label[pred]\n",
    "    if label !=0:\n",
    "        start,end = offsets[idx]\n",
    "        results.append(\n",
    "            {'entity':label,\n",
    "        'score':probabilities[idx][pred],\n",
    "        'word':tokens[idx],\n",
    "        'start':start,\n",
    "        'end':end}\n",
    "        )"
   ],
   "id": "7822e88d52fe14ce",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:57:37.733084Z",
     "start_time": "2025-10-28T17:57:37.728083Z"
    }
   },
   "cell_type": "code",
   "source": "print(results)",
   "id": "189a76ed015fca4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'O', 'score': 0.9996232986450195, 'word': '[CLS]', 'start': 0, 'end': 0}, {'entity': 'I-PER', 'score': 0.999491810798645, 'word': 'Da', 'start': 0, 'end': 2}, {'entity': 'I-PER', 'score': 0.999091625213623, 'word': '##wood', 'start': 2, 'end': 6}, {'entity': 'O', 'score': 0.999958872795105, 'word': 'was', 'start': 7, 'end': 10}, {'entity': 'O', 'score': 0.9999486207962036, 'word': 'working', 'start': 11, 'end': 18}, {'entity': 'O', 'score': 0.9998605251312256, 'word': 'for', 'start': 19, 'end': 22}, {'entity': 'I-ORG', 'score': 0.9988266825675964, 'word': 'UN', 'start': 23, 'end': 25}, {'entity': 'O', 'score': 0.9997767806053162, 'word': 'in', 'start': 26, 'end': 28}, {'entity': 'I-LOC', 'score': 0.9998264908790588, 'word': 'Pakistan', 'start': 29, 'end': 37}, {'entity': 'O', 'score': 0.9996232986450195, 'word': '[SEP]', 'start': 0, 'end': 0}]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T17:57:57.387528Z",
     "start_time": "2025-10-28T17:57:57.382528Z"
    }
   },
   "cell_type": "code",
   "source": "sentence[29:37]",
   "id": "72228c0c1c95ac20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pakistan'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "809ebe453b34f627"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
