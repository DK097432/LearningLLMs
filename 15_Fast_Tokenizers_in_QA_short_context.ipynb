{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T08:47:55.146555Z",
     "start_time": "2025-10-29T08:47:36.134516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")"
   ],
   "id": "65423d65c3db604f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:36:27.465016Z",
     "start_time": "2025-10-29T09:36:27.169116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "context =  \"\"\"\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = 'which deef learning libraries back Transformers?'\n",
    "question_answerer(question = question, context = context)"
   ],
   "id": "a951b0a55c18b072",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.11953046917915344,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:36:51.990812Z",
     "start_time": "2025-10-29T09:36:51.984816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "long_context = \"\"\"\n",
    "ðŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ðŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ðŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ðŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\""
   ],
   "id": "246dff1c247082f8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:37:09.718367Z",
     "start_time": "2025-10-29T09:37:09.579839Z"
    }
   },
   "cell_type": "code",
   "source": "question_answerer(question = question, context = long_context)",
   "id": "5a5f3a30c246aa44",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.19243748486042023,\n",
       " 'start': 1847,\n",
       " 'end': 1889,\n",
       " 'answer': 'three most popular deep learning libraries'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:38:16.598783Z",
     "start_time": "2025-10-29T09:38:16.595780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now we will manually do the question answering\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ],
   "id": "8c3ad043b6d0c59b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:40:58.478842Z",
     "start_time": "2025-10-29T09:38:54.693261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = 'distilbert-base-cased-distilled-squad'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)"
   ],
   "id": "ddc7ad10094dfa3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2b4d5bfb3934586907571a73109ad2c"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dawood Khan\\anaconda3\\envs\\llm\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dawood Khan\\.cache\\huggingface\\hub\\models--distilbert-base-cased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:41:01.730955Z",
     "start_time": "2025-10-29T09:41:01.619915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inputs = tokenizer(question, context, return_tensors= 'pt')\n",
    "outputs = model(**inputs)"
   ],
   "id": "43dc4d96ee071965",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T09:41:35.856745Z",
     "start_time": "2025-10-29T09:41:35.852236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(start_logits.shape, end_logits.shape)"
   ],
   "id": "989f4e7386f03fc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 67]) torch.Size([1, 67])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:10:21.924750Z",
     "start_time": "2025-10-29T10:10:21.917238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask Everything apart from the tokens of the context\n",
    "mask = [i !=1 for i in sequence_ids]\n",
    "# Unmask the CLS Token as it is needed\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]\n",
    "\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000"
   ],
   "id": "eb4b79d1821fb0c3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:15:47.576874Z",
     "start_time": "2025-10-29T10:15:47.572223Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we will apply softmax to turn logits in to probabilities\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)[0]\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)[0]"
   ],
   "id": "c731cd53ad47d83c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:25:27.864832Z",
     "start_time": "2025-10-29T10:25:27.859830Z"
    }
   },
   "cell_type": "code",
   "source": "scores = start_probabilities[:,None] * end_probabilities[None,:]",
   "id": "da295a212d32285a",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:25:28.256705Z",
     "start_time": "2025-10-29T10:25:28.251701Z"
    }
   },
   "cell_type": "code",
   "source": "print(scores)",
   "id": "b2292dac4d3a1a6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3735e-09, 2.0358e-09,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3689e-09, 1.1741e-09,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:25:30.244772Z",
     "start_time": "2025-10-29T10:25:30.239773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# mask the values where start_index > end_index by setting them to 0\n",
    "\n",
    "scores = torch.triu(scores)"
   ],
   "id": "86125a0f69221da4",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:25:30.526333Z",
     "start_time": "2025-10-29T10:25:30.521331Z"
    }
   },
   "cell_type": "code",
   "source": "scores.shape",
   "id": "6adb3eac0b24af18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65, 65])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:25:32.062815Z",
     "start_time": "2025-10-29T10:25:32.056301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we will get the index of the maximum\n",
    "max_index = scores.argmax().item()\n",
    "start_index = max_index//scores.shape[1]\n",
    "end_index= max_index % scores.shape[1]\n",
    "\n",
    "print(scores[start_index,end_index])"
   ],
   "id": "b4f23575f89e0ac2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6609, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:28:48.597Z",
     "start_time": "2025-10-29T10:28:48.591608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now we need to convert character indices into context\n",
    "inputs_with_offsets = tokenizer(question, context, return_offsets_mapping = True)\n",
    "offsets = inputs_with_offsets['offset_mapping']\n",
    "\n",
    "start_char, _ = offsets[start_index]\n",
    "_, end_char = offsets[end_index]\n",
    "answer = context[start_char:end_char]"
   ],
   "id": "ae5f396eff7d5275",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:29:30.260102Z",
     "start_time": "2025-10-29T10:29:30.253384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = {\n",
    "    'answer': answer,\n",
    "    'start':start_char,\n",
    "    'end':end_char,\n",
    "    'score':scores[start_index,end_index],\n",
    "}\n",
    "print(result)"
   ],
   "id": "c4b256a9dcb3376b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'by the three most popular deep learning libraries â€” Jax, PyTorch, and Tensor', 'start': 26, 'end': 102, 'score': tensor(0.6609, grad_fn=<SelectBackward0>)}\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:35:17.828198Z",
     "start_time": "2025-10-29T10:35:17.824688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now if we want top 5 Answers for our question\n",
    "top_k= 5\n",
    "flat_scores = scores.flatten()"
   ],
   "id": "dbec5e42b6b73c34",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:37:59.975200Z",
     "start_time": "2025-10-29T10:37:59.965187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we will get the topindices and the top 5 socres\n",
    "top_scores, top_indices = torch.topk(flat_scores, k = top_k)\n",
    "# Convert the flat indices back to start and end indices\n",
    "start_indices = (top_indices//scores.shape[1]).tolist()\n",
    "end_indices = (top_indices%scores.shape[1]).tolist()"
   ],
   "id": "69a80278dab4104e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:40:58.828779Z",
     "start_time": "2025-10-29T10:40:58.821781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we'll extract top 5 answers\n",
    "answers = []\n",
    "for s_idx,e_idx, score in zip(start_indices, end_indices, top_scores):\n",
    "    start_char, _ = offsets[s_idx]\n",
    "    _, end_char  = offsets[e_idx]\n",
    "    answer = context[start_char:end_char]\n",
    "    answers.append(\n",
    "        {\n",
    "            'answer': answer,\n",
    "            'start':start_char,\n",
    "            'end':end_char,\n",
    "            'score':score.item(),\n",
    "        }\n",
    "    )\n",
    "\n"
   ],
   "id": "17a8d0ec21267502",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T10:41:43.889718Z",
     "start_time": "2025-10-29T10:41:43.883718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now sort the answers\n",
    "answers  = sorted(answers, key = lambda x:x['score'], reverse = True)\n",
    "print(answers)"
   ],
   "id": "e8b6e31708b18724",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'answer': 'by the three most popular deep learning libraries â€” Jax, PyTorch, and Tensor', 'start': 26, 'end': 102, 'score': 0.660897970199585}, {'answer': 'libraries â€” Jax, PyTorch, and Tensor', 'start': 66, 'end': 102, 'score': 0.11953002214431763}, {'answer': 'by', 'start': 26, 'end': 28, 'score': 0.040866415947675705}, {'answer': 'by the three most popular deep', 'start': 26, 'end': 56, 'score': 0.03716517984867096}, {'answer': 'backed by the three most popular deep learning libraries â€” Jax, PyTorch, and Tensor', 'start': 19, 'end': 102, 'score': 0.02173474244773388}]\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "21835123d814be08"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
