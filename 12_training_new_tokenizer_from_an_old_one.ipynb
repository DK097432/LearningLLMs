{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-28T09:34:42.039949Z",
     "start_time": "2025-10-28T09:34:37.814923Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:34:51.111460Z",
     "start_time": "2025-10-28T09:34:42.097486Z"
    }
   },
   "cell_type": "code",
   "source": "raw_dataset = load_dataset(\"Nan-Do/code-search-net-python\")",
   "id": "73f3fdfc44c32b52",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:34:52.721979Z",
     "start_time": "2025-10-28T09:34:52.715976Z"
    }
   },
   "cell_type": "code",
   "source": "raw_dataset['train']",
   "id": "1d6373de373b187c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url', 'partition', 'summary'],\n",
       "    num_rows: 455243\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:01.430747Z",
     "start_time": "2025-10-28T09:35:01.423748Z"
    }
   },
   "cell_type": "code",
   "source": "raw_dataset['train'][123543]['original_string']",
   "id": "8795e104c9665836",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def _api_on_write_error(self, status_code, **kwargs):\\n        \"\"\"\\n        Catches errors and renders it as a JSON message. Adds the traceback if\\n        debug is enabled.\\n        \"\"\"\\n\\n        return_error = { \"code\": self.get_status() }\\n        exc_info = kwargs.get(\"exc_info\")\\n\\n        if exc_info and isinstance(exc_info[1], oz.json_api.ApiError):\\n            return_error[\"error\"] = exc_info[1].message\\n        else:\\n            return_error[\"error\"] = API_ERROR_CODE_MAP.get(self.get_status(), \"Unknown error\")\\n\\n        if oz.settings.get(\"debug\"):\\n            return_error[\"trace\"] = \"\".join(traceback.format_exception(*exc_info))\\n\\n        self.finish(return_error)\\n        return oz.break_trigger'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:01.438750Z",
     "start_time": "2025-10-28T09:35:01.432750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we turn the data into lists so that tokenizer would train fast\n",
    "training_corpus = (\n",
    "    raw_dataset['train'][i:i+1000]['original_string']\n",
    "    for i in range(0, len(raw_dataset['train']),1000)\n",
    ")# only 1000 texts at a time will be loaded. Thus our memory won't be exhausted while processing"
   ],
   "id": "fec1bf35c73bdaa6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:01.446749Z",
     "start_time": "2025-10-28T09:35:01.440747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gen = (i for i in range(10))\n",
    "print(list(gen))\n",
    "print(list(gen)) # generator function return the value once only which is a problem"
   ],
   "id": "47f82fe96c9e952b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:01.614747Z",
     "start_time": "2025-10-28T09:35:01.608752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Thus, we define a function that return a generator instead\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_dataset['train'][i:i+1000]['original_string']\n",
    "        for i in range(0,len(raw_dataset['train']), 1000)\n",
    "    )"
   ],
   "id": "3fecf1605f90132e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:01.619751Z",
     "start_time": "2025-10-28T09:35:01.615751Z"
    }
   },
   "cell_type": "code",
   "source": "training_corpus = get_training_corpus()",
   "id": "55a18d5879d5f3a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:01.646749Z",
     "start_time": "2025-10-28T09:35:01.641748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We can also use for loop for the above function\n",
    "def get_training_corpus():\n",
    "    dataset = raw_dataset['train']\n",
    "    for start_idx in range(0,len(dataset),1000):\n",
    "        samples = dataset[start_idx:start_idx+1000]\n",
    "        yield  samples['original_string']"
   ],
   "id": "146d754250fd78e9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training A New Tokenizer",
   "id": "90abc767e840bcef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:07.616109Z",
     "start_time": "2025-10-28T09:35:01.678749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained('gpt2')"
   ],
   "id": "b300565fb4d9b7e0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:35:07.640113Z",
     "start_time": "2025-10-28T09:35:07.633109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's look at the tokenization of the old tokenizer\n",
    "sentences = '''def addition(x,y):\n",
    "    Adding two numbers x and y\n",
    "        return x+y'''\n",
    "\n",
    "tokens = old_tokenizer.tokenize(sentences)\n",
    "print(tokens)"
   ],
   "id": "369b5ec5c49c9301",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġaddition', '(', 'x', ',', 'y', '):', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'ĠAdding', 'Ġtwo', 'Ġnumbers', 'Ġx', 'Ġand', 'Ġy', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġx', '+', 'y']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:37:25.889140Z",
     "start_time": "2025-10-28T09:35:07.642112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now we train the tokenizer on our training corpus\n",
    "new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus,52000)"
   ],
   "id": "a5395c018da5b4c6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:39:31.458676Z",
     "start_time": "2025-10-28T09:39:31.453676Z"
    }
   },
   "cell_type": "code",
   "source": "new_tokens = new_tokenizer.tokenize(sentences)",
   "id": "7ed334a3faf258b4",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:39:37.623856Z",
     "start_time": "2025-10-28T09:39:37.617860Z"
    }
   },
   "cell_type": "code",
   "source": "print(new_tokens)",
   "id": "42fdece2731450a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def', 'Ġaddition', '(', 'x', ',', 'y', '):', 'ĊĠĠĠ', 'ĠAdding', 'Ġtwo', 'Ġnumbers', 'Ġx', 'Ġand', 'Ġy', 'ĊĠĠĠĠĠĠĠ', 'Ġreturn', 'Ġx', '+', 'y']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:40:17.215180Z",
     "start_time": "2025-10-28T09:40:17.210673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# see the length of tokens here\n",
    "print(len(tokens))\n",
    "print(len(new_tokens))"
   ],
   "id": "4781238dac7c053a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "19\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:46:52.876340Z",
     "start_time": "2025-10-28T09:46:52.824703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Now save the tokenizer\n",
    "new_tokenizer.save_pretrained('code_search_net_tokenizer_gpt2')"
   ],
   "id": "49eb163afc5e171e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('code_search_net_tokenizer_gpt2\\\\tokenizer_config.json',\n",
       " 'code_search_net_tokenizer_gpt2\\\\special_tokens_map.json',\n",
       " 'code_search_net_tokenizer_gpt2\\\\vocab.json',\n",
       " 'code_search_net_tokenizer_gpt2\\\\merges.txt',\n",
       " 'code_search_net_tokenizer_gpt2\\\\added_tokens.json',\n",
       " 'code_search_net_tokenizer_gpt2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:46:53.069984Z",
     "start_time": "2025-10-28T09:46:53.052471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "d063ae1781217204",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4aa25104c9fc4c979d3eca6762a9f571"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-28T09:48:36.875982Z",
     "start_time": "2025-10-28T09:47:49.087076Z"
    }
   },
   "cell_type": "code",
   "source": "!huggingface-cli login",
   "id": "86035776f35d0474",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "new_tokenizer.push_to_hub('code_search_net_tokenizer_gpt2')",
   "id": "6f51d7ea7d638468"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
